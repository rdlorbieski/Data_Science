{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03_02 Preparing Text for Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample hashtag data :\n",
      "                            Course                             HashTags\n",
      "0  Apache Spark Essential Training  BigData,DataScience,MachineLearning\n",
      "1           Java Memory Management            Java,Advanced,Programming\n",
      "\n",
      " Feature names Identified :\n",
      "\n",
      "['advanced', 'automation', 'bigdata', 'datascience', 'design', 'developer', 'gcp', 'graphics', 'hadoop', 'ide', 'intermediate', 'java', 'jdbc', 'kubernetes', 'machinelearning', 'patterns', 'programming', 'python', 'scala', 'scripting']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#Load course hashtags\n",
    "hashtags_df=pd.read_csv(\"Course-Hashtags.csv\")\n",
    "print(\"\\nSample hashtag data :\")\n",
    "print(hashtags_df[:2])\n",
    "\n",
    "#Seperate Hashtags and titles to lists\n",
    "hash_list = hashtags_df[\"HashTags\"].tolist()\n",
    "title_list = hashtags_df[\"Course\"].tolist()\n",
    "\n",
    "#Do TF-IDF conversion of hashtags\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#Para utilizar a clusterização é necessário transformar os dados em números:\n",
    "#O tf-idf ajuda a dizer a relevância (com um peso) de certa palavra em certo contexto\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "hash_matrix=vectorizer.fit_transform(hash_list)\n",
    "print(\"\\n Feature names Identified :\\n\")\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03_03 Clustering TF-IDF data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Group :  0 \n",
      "-------------------\n",
      "Apache Spark Essential Training\n",
      "Machine Learning and AI Foundations\n",
      "Hadoop for Data Science\n",
      "Data Science on Google Cloud Platform\n",
      "Scala for Data Science\n",
      "\n",
      "Group :  1 \n",
      "-------------------\n",
      "Java Memory Management\n",
      "Java : Database Integration and JDBC\n",
      "R Programming\n",
      "Java IDE Overview\n",
      "Kubernetes for Java Developers\n",
      "\n",
      "Group :  2 \n",
      "-------------------\n",
      "Python Automation and Testing\n",
      "Python for Graphics\n",
      "Python Design Patterns\n",
      "Python Scripting\n"
     ]
    }
   ],
   "source": [
    "#Use KMeans clustering from scikit-learn\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "#Split data into 3 clusters\n",
    "kmeans = KMeans(n_clusters=3).fit(hash_matrix) # define em qual dos 3 clusters para cada palavra pertence\n",
    "\n",
    "#get Cluster labels.\n",
    "clusters=kmeans.labels_\n",
    "\n",
    "#Print cluster label and Courses under each cluster\n",
    "for group in set(clusters):\n",
    "    print(\"\\nGroup : \",group, \"\\n-------------------\")\n",
    "    \n",
    "    for i in hashtags_df.index:\n",
    "        if ( clusters[i] == group):\n",
    "            print(title_list[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03_04 Finding optimal Cluster size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of squared distances :  [11.433022387031057, 9.055813116540266, 6.761066172191217, 5.628321226007316, 4.688883377604404, 4.111018059807071, 3.3481406712109574, 3.0519887306312783, 2.3372944536391906, 1.808454151494424, 1.3264482360674248, 0.8457910455266844, 0.40164181287464085, 0.0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Find optimal cluster size by finding sum-of-squared-distances\n",
    "\n",
    "sosd = []\n",
    "#Run clustering for sizes 1 to 15 and capture inertia\n",
    "K = range(1,15)\n",
    "for k in K:\n",
    "    km = KMeans(n_clusters=k)\n",
    "    km = km.fit(hash_matrix)\n",
    "    sosd.append(km.inertia_)\n",
    "    \n",
    "print(\"Sum of squared distances : \" ,sosd)\n",
    "\n",
    "#Plot sosd against number of clusters\n",
    "import matplotlib.pyplot as mpLib\n",
    "mpLib.plot(K, sosd, 'bx-')\n",
    "mpLib.xlabel('Cluster count')\n",
    "mpLib.ylabel('Sum_of_squared_distances')\n",
    "mpLib.title('Elbow Method For Optimal Cluster Size')\n",
    "mpLib.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
