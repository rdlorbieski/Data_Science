{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataframe rodando query SQL\n",
    "### Dados relacionais com flexível e poderoso analytics\n",
    "\n",
    "Armazenamentos de dados relacionais são fáceis de construir e consultar. Usuários e desenvolvedores geralmente preferem escrever consultas declarativas e fáceis de interpretar em uma linguagem legível semelhante a humanos, como SQL. No entanto, conforme os dados começam a aumentar em volume e variedade, a abordagem relacional não é dimensionada bem o suficiente para construir aplicativos de Big Data e sistemas analíticos. A seguir estão alguns dos principais desafios.\n",
    "\n",
    "* Lidar com diferentes tipos e fontes de dados, que podem ser estruturados, semiestruturados e não estruturados\n",
    "* Construir pipelines de ETL de e para várias fontes de dados, o que pode levar ao desenvolvimento de muitos códigos personalizados específicos, aumentando assim o débito técnico ao longo do tempo\n",
    "* Ter a capacidade de realizar análises baseadas em business intelligence (BI) tradicional e análises avançadas (aprendizado de máquina, modelagem estatística, etc.), a última das quais é definitivamente desafiadora para executar em sistemas relacionais\n",
    "\n",
    "Tivemos sucesso no domínio da análise de Big Data com Hadoop e o paradigma MapReduce. Isso era poderoso, mas geralmente lento, e dava aos usuários uma interface de programação procedural de baixo nível que exigia que as pessoas escrevessem muitos códigos até mesmo para transformações de dados muito simples. No entanto, uma vez que o Spark foi lançado, ele realmente revolucionou a forma como a análise de Big Data era feita com foco na computação in-memory, tolerância a falhas, abstrações de alto nível e facilidade de uso.\n",
    "\n",
    "A partir de então, várias estruturas e sistemas como Hive, Pig e Shark (que evoluíram para Spark SQL) forneceram interfaces relacionais ricas e mecanismos de consulta declarativa para armazenamentos de Big Data. O desafio era que essas ferramentas eram relacionais ou baseadas em procedimentos, e não poderíamos ter o melhor dos dois mundos.\n",
    "\n",
    "![spark-1](https://opensource.com/sites/default/files/uploads/2_hadoop-vs-spark.png)\n",
    "\n",
    "No entanto, no mundo real, a maioria dos pipelines de dados e analíticos pode envolver uma combinação de código relacional e procedural. Forçar os usuários a escolher um deles acaba complicando as coisas e aumentando os esforços do usuário no desenvolvimento, construção e manutenção de diferentes aplicativos e sistemas. O Apache Spark SQL se baseia no esforço SQL-on-Spark mencionado anteriormente, chamado Shark. Em vez de forçar os usuários a escolher entre uma API relacional ou procedural, o Spark SQL tenta permitir que os usuários combinem perfeitamente as duas e executem consulta, recuperação e análise de dados em escala no Big Data.\n",
    "\n",
    "### Spark SQL\n",
    "O Spark SQL basicamente tenta preencher a lacuna entre os dois modelos que mencionamos anteriormente - os modelos relacionais e procedurais.\n",
    "\n",
    "Spark SQL fornece uma API DataFrame que pode executar operações relacionais em fontes de dados externas e coleções distribuídas integradas do Spark - em escala!\n",
    "\n",
    "Para oferecer suporte a uma ampla variedade de fontes de dados e algoritmos em Big Data, o Spark SQL apresenta um novo otimizador extensível chamado Catalyst, que torna mais fácil adicionar fontes de dados, regras de otimização e tipos de dados para análises avançadas, como aprendizado de máquina.\n",
    "Essencialmente, o Spark SQL aproveita o poder do Spark para realizar cálculos distribuídos e robustos na memória em grande escala no Big Data.\n",
    "\n",
    "Spark SQL fornece desempenho SQL de última geração e também mantém a compatibilidade com todas as estruturas e componentes existentes suportados pelo Apache Hive (uma estrutura de armazenamento de Big Data popular), incluindo formatos de dados, funções definidas pelo usuário (UDFs) e o metastore. Além disso, ele também ajuda a ingerir uma ampla variedade de formatos de dados de fontes de Big Data e data warehouses corporativos como JSON, Hive, Parquet e assim por diante, e realizar uma combinação de operações relacionais e procedimentais para análises mais complexas e avançadas.\n",
    "\n",
    "\n",
    "![Spark-2](https://cdn-images-1.medium.com/max/2000/1*OY41hGbe4IB9-hHLRPuCHQ.png)\n",
    "\n",
    "### Spark SQL with Dataframe API is fast\n",
    "O Spark SQL demonstrou ser extremamente rápido, até mesmo comparável aos mecanismos baseados em C ++, como o Impala.\n",
    "\n",
    "![spark_speed](https://opensource.com/sites/default/files/uploads/9_spark-dataframes-vs-rdds-and-sql.png)\n",
    "\n",
    "O gráfico a seguir mostra um bom resultado de benchmark de DataFrames vs. RDDs em diferentes linguagens, o que dá uma perspectiva interessante sobre como os DataFrames podem ser otimizados.\n",
    "\n",
    "![spark-speed-2](https://opensource.com/sites/default/files/uploads/10_comparing-spark-dataframes-and-rdds.png)\n",
    "\n",
    "Por que o Spark SQL é tão rápido e otimizado? O motivo é por causa de um novo otimizador extensível, ** Catalyst **, baseado em construções de programação funcional em Scala.\n",
    "\n",
    "O design extensível do Catalyst tem duas finalidades.\n",
    "\n",
    "* Facilita a adição de novas técnicas e recursos de otimização ao Spark SQL, especialmente para lidar com diversos problemas em torno de Big Data, dados semiestruturados e análises avançadas\n",
    "* Facilidade de poder estender o otimizador - por exemplo, adicionando regras específicas da fonte de dados que podem enviar filtragem ou agregação em sistemas de armazenamento externo ou suporte para novos tipos de dados\n",
    "\n",
    "O Catalyst oferece suporte à otimização baseada em regras e em custos. Embora os otimizadores extensíveis tenham sido propostos no passado, eles normalmente exigiam uma linguagem complexa de domínio específico para especificar regras. Normalmente, isso leva a uma curva de aprendizado significativa e carga de manutenção. Em contraste, o Catalyst usa recursos padrão da linguagem de programação Scala, como correspondência de padrões, para permitir que os desenvolvedores usem a linguagem de programação completa, ao mesmo tempo que facilita a especificação das regras.\n",
    "\n",
    "![catalyst-2](https://cdn-images-1.medium.com/max/1500/1*81ZOMxCci-tM2b-HNUX6Ww.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful references for this Notebook\n",
    "* [PySpark in Jupyter Notebook — Working with Dataframe & JDBC Data Sources](https://medium.com/@thucnc/pyspark-in-jupyter-notebook-working-with-dataframe-jdbc-data-sources-6f3d39300bf6)\n",
    "* [PySpark - Working with JDBC Sqlite database](http://mitzen.blogspot.com/2017/06/pyspark-working-with-jdbc-sqlite.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a SparkSession and read the a stock price dataset CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext as sc\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark1 = SparkSession.builder.appName('SQL').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark1.read.csv('Data/appl_stock.csv',inferSchema=True,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- Volume: integer (nullable = true)\n",
      " |-- Adj Close: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a temporary view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('stock')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now run a simple SQL query directly on this view. It returns a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Date: string, Open: double, High: double, Low: double, Close: double, Volume: int, Adj Close: double]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = spark1.sql(\"SELECT * FROM stock LIMIT 5\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Adj Close']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+------------------+------------------+---------+------------------+\n",
      "|      Date|      Open|      High|               Low|             Close|   Volume|         Adj Close|\n",
      "+----------+----------+----------+------------------+------------------+---------+------------------+\n",
      "|2010-01-04|213.429998|214.499996|212.38000099999996|        214.009998|123432400|         27.727039|\n",
      "|2010-01-05|214.599998|215.589994|        213.249994|        214.379993|150476200|27.774976000000002|\n",
      "|2010-01-06|214.379993|    215.23|        210.750004|        210.969995|138040000|27.333178000000004|\n",
      "|2010-01-07|    211.75|212.000006|        209.050005|            210.58|119282800|          27.28265|\n",
      "|2010-01-08|210.299994|212.000006|209.06000500000002|211.98000499999998|111902700|         27.464034|\n",
      "+----------+----------+----------+------------------+------------------+---------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run slightly more complex queries\n",
    "How many entries in the `Close` field are higher than 500?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|count(Close)|\n",
      "+------------+\n",
      "|         403|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "count_greater_500 = spark1.sql(\"SELECT COUNT(Close) FROM stock WHERE Close > 500\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the average `Open` values of all the entries where `Volume` is either greater than 120 million or less than 110 million?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|         avg(Open)|\n",
      "+------------------+\n",
      "|309.12406365290224|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "avg_1 = spark1.sql(\"SELECT AVG(Open) FROM stock WHERE Volume > 120000000 OR Volume < 110000000\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read a file (and create dataframe) by directly running a `spark.sql` method on the file\n",
    "Notice the syntax of `csv.<path->filename.csv>` inside the SQL query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales = spark1.sql(\"SELECT * FROM csv.`Data/sales_info.csv`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+\n",
      "|    _c0|    _c1|  _c2|\n",
      "+-------+-------+-----+\n",
      "|Company| Person|Sales|\n",
      "|   GOOG|    Sam|  200|\n",
      "|   GOOG|Charlie|  120|\n",
      "|   GOOG|  Frank|  340|\n",
      "|   MSFT|   Tina|  600|\n",
      "|   MSFT|    Amy|  124|\n",
      "|   MSFT|Vanessa|  243|\n",
      "|     FB|   Carl|  870|\n",
      "|     FB|  Sarah|  350|\n",
      "|   APPL|   John|  250|\n",
      "|   APPL|  Linda|  130|\n",
      "|   APPL|   Mike|  750|\n",
      "|   APPL|  Chris|  350|\n",
      "+-------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sales.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read tables from a local SQLite database file using `JDBC` connection\n",
    "For this read, we will use the famous chinook DB in SQLite tutorial. You can [download the file from here](http://www.sqlitetutorial.net/sqlite-sample-database/). Remember to unzip it in a suitable directory. You will need the path to this file later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First `cd` to the directory where all the PySpark jar files are kept. Then download the SQLite jar file from the [given URL](https://mvnrepository.com/artifact/org.xerial/sqlite-jdbc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd /home/tirtha/Spark/spark-2.3.1-bin-hadoop2.7/jars/\n",
    "!curl https://repo1.maven.org/maven2/org/xerial/sqlite-jdbc/3.28.0/sqlite-jdbc-3.28.0.jar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define driver, path to local .db file, and append that path to `jdbc:sqlite` to construct the `url`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = \"org.sqlite.JDBC\"\n",
    "path = \"Data/chinook.db\"\n",
    "url = \"jdbc:sqlite:\" + path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define `tablename` to be read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tablename = \"albums\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "from pyspark.sql import SQLContext\n",
    "sqlCtx = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_albums = spark1.read.format(\"jdbc\").option(\"url\", url).option(\"dbtable\", tablename).option(\"driver\", driver).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------+\n",
      "|AlbumId|               Title|ArtistId|\n",
      "+-------+--------------------+--------+\n",
      "|      1|For Those About T...|       1|\n",
      "|      2|   Balls to the Wall|       2|\n",
      "|      3|   Restless and Wild|       2|\n",
      "|      4|   Let There Be Rock|       1|\n",
      "|      5|            Big Ones|       3|\n",
      "|      6|  Jagged Little Pill|       4|\n",
      "|      7|            Facelift|       5|\n",
      "|      8|      Warner 25 Anos|       6|\n",
      "|      9|Plays Metallica B...|       7|\n",
      "|     10|          Audioslave|       8|\n",
      "|     11|        Out Of Exile|       8|\n",
      "|     12| BackBeat Soundtrack|       9|\n",
      "|     13|The Best Of Billy...|      10|\n",
      "|     14|Alcohol Fueled Br...|      11|\n",
      "|     15|Alcohol Fueled Br...|      11|\n",
      "|     16|       Black Sabbath|      12|\n",
      "|     17|Black Sabbath Vol...|      12|\n",
      "|     18|          Body Count|      13|\n",
      "|     19|    Chemical Wedding|      14|\n",
      "|     20|The Best Of Buddy...|      15|\n",
      "+-------+--------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_albums.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- AlbumId: integer (nullable = true)\n",
      " |-- Title: string (nullable = true)\n",
      " |-- ArtistId: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_albums.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Don't forget to create temp view to use later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_albums.createTempView('albums')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read another table - `artists`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tablename = \"artists\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_artists = spark1.read.format(\"jdbc\").option(\"url\", url).option(\"dbtable\", tablename).option(\"driver\", driver).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+\n",
      "|ArtistId|                Name|\n",
      "+--------+--------------------+\n",
      "|       1|               AC/DC|\n",
      "|       2|              Accept|\n",
      "|       3|           Aerosmith|\n",
      "|       4|   Alanis Morissette|\n",
      "|       5|     Alice In Chains|\n",
      "|       6|Antônio Carlos Jobim|\n",
      "|       7|        Apocalyptica|\n",
      "|       8|          Audioslave|\n",
      "|       9|            BackBeat|\n",
      "|      10|        Billy Cobham|\n",
      "|      11| Black Label Society|\n",
      "|      12|       Black Sabbath|\n",
      "|      13|          Body Count|\n",
      "|      14|     Bruce Dickinson|\n",
      "|      15|           Buddy Guy|\n",
      "|      16|      Caetano Veloso|\n",
      "|      17|       Chico Buarque|\n",
      "|      18|Chico Science & N...|\n",
      "|      19|        Cidade Negra|\n",
      "|      20|        Cláudio Zoli|\n",
      "+--------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_artists.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_artists.createTempView('artists')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test if SQL query is working fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+\n",
      "|ArtistId|     Name|\n",
      "+--------+---------+\n",
      "|       1|    AC/DC|\n",
      "|       2|   Accept|\n",
      "|       3|Aerosmith|\n",
      "|       9| BackBeat|\n",
      "|      15|Buddy Guy|\n",
      "|      26|  Azymuth|\n",
      "|      36|  O Rappa|\n",
      "|      37| Ed Motta|\n",
      "|      46|Jorge Ben|\n",
      "|      50|Metallica|\n",
      "+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark1.sql(\"SELECT * FROM artists WHERE length(Name)<10 LIMIT 10\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join the `albums` and `artists` tables in a single dataframe using SQL query (and order by `ArtistId`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined = spark1.sql(\"SELECT * FROM artists LEFT JOIN albums ON artists.ArtistId=albums.ArtistId order by artists.ArtistId\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-------+--------------------+--------+\n",
      "|ArtistId|                Name|AlbumId|               Title|ArtistId|\n",
      "+--------+--------------------+-------+--------------------+--------+\n",
      "|       1|               AC/DC|      1|For Those About T...|       1|\n",
      "|       1|               AC/DC|      4|   Let There Be Rock|       1|\n",
      "|       2|              Accept|      2|   Balls to the Wall|       2|\n",
      "|       2|              Accept|      3|   Restless and Wild|       2|\n",
      "|       3|           Aerosmith|      5|            Big Ones|       3|\n",
      "|       4|   Alanis Morissette|      6|  Jagged Little Pill|       4|\n",
      "|       5|     Alice In Chains|      7|            Facelift|       5|\n",
      "|       6|Antônio Carlos Jobim|      8|      Warner 25 Anos|       6|\n",
      "|       6|Antônio Carlos Jobim|     34|Chill: Brazil (Di...|       6|\n",
      "|       7|        Apocalyptica|      9|Plays Metallica B...|       7|\n",
      "|       8|          Audioslave|    271|         Revelations|       8|\n",
      "|       8|          Audioslave|     11|        Out Of Exile|       8|\n",
      "|       8|          Audioslave|     10|          Audioslave|       8|\n",
      "|       9|            BackBeat|     12| BackBeat Soundtrack|       9|\n",
      "|      10|        Billy Cobham|     13|The Best Of Billy...|      10|\n",
      "|      11| Black Label Society|     14|Alcohol Fueled Br...|      11|\n",
      "|      11| Black Label Society|     15|Alcohol Fueled Br...|      11|\n",
      "|      12|       Black Sabbath|     17|Black Sabbath Vol...|      12|\n",
      "|      12|       Black Sabbath|     16|       Black Sabbath|      12|\n",
      "|      13|          Body Count|     18|          Body Count|      13|\n",
      "+--------+--------------------+-------+--------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_combined.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's the difference between temporary and global SQL views? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A temporary view does not persist (shared) across multiple sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+\n",
      "|ArtistId|                Name|\n",
      "+--------+--------------------+\n",
      "|       1|               AC/DC|\n",
      "|       2|              Accept|\n",
      "|       3|           Aerosmith|\n",
      "|       4|   Alanis Morissette|\n",
      "|       5|     Alice In Chains|\n",
      "|       6|Antônio Carlos Jobim|\n",
      "|       7|        Apocalyptica|\n",
      "|       8|          Audioslave|\n",
      "|       9|            BackBeat|\n",
      "|      10|        Billy Cobham|\n",
      "+--------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_artists.createOrReplaceTempView(\"temp_artists\")\n",
    "\n",
    "df_temp = spark1.sql(\"SELECT * FROM temp_artists LIMIT 10\")\n",
    "df_temp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A new session is created but the temp view `temp_artists` cannot be accessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark2 = SparkSession.builder.appName('SQL2').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We use `try...except` to catch the error and display a generic message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df_temp = spark2.sql(\"SELECT * FROM temp_artists LIMIT 10\")\n",
    "except:\n",
    "    print(\"Error happened in this execution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, a global view is created in this session\n",
    "Global temporary view is tied to a system preserved database `global_temp`. So the view name must be referenced as such."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tablename = \"artists\"\n",
    "df_artists = spark2.read.format(\"jdbc\").option(\"url\", url).option(\"dbtable\", tablename).option(\"driver\", driver).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+\n",
      "|ArtistId|                Name|\n",
      "+--------+--------------------+\n",
      "|       1|               AC/DC|\n",
      "|       2|              Accept|\n",
      "|       3|           Aerosmith|\n",
      "|       4|   Alanis Morissette|\n",
      "|       5|     Alice In Chains|\n",
      "|       6|Antônio Carlos Jobim|\n",
      "|       7|        Apocalyptica|\n",
      "|       8|          Audioslave|\n",
      "|       9|            BackBeat|\n",
      "|      10|        Billy Cobham|\n",
      "+--------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_artists.createOrReplaceGlobalTempView(\"global_artists\")\n",
    "\n",
    "df_global = spark2.sql(\"SELECT * FROM global_temp.global_artists LIMIT 10\")\n",
    "df_global.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start a new session. The view `global_artists` can be accessed across the sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark3 = SparkSession.builder.appName('SQL3').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+\n",
      "|ArtistId|                Name|\n",
      "+--------+--------------------+\n",
      "|       1|               AC/DC|\n",
      "|       2|              Accept|\n",
      "|       3|           Aerosmith|\n",
      "|       4|   Alanis Morissette|\n",
      "|       5|     Alice In Chains|\n",
      "|       6|Antônio Carlos Jobim|\n",
      "|       7|        Apocalyptica|\n",
      "|       8|          Audioslave|\n",
      "|       9|            BackBeat|\n",
      "|      10|        Billy Cobham|\n",
      "+--------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_global = spark3.sql(\"SELECT * FROM global_temp.global_artists LIMIT 10\")\n",
    "df_global.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
